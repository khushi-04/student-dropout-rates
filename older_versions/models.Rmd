---
title: "STAT167 Project"
output:
  pdf_document: default
  html_notebook: default
editor_options:
  chunk_output_type: inline
---

## Libraries
```{r}
library(tidyverse) ## For 'dplyr' & 'ggplot'
library(gridExtra)
```


```{r}
### The dataset was downloaded from this link:  
### https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success
df <- read.csv("all_data.csv", header = TRUE, sep = ';')
```
## We are interested in understanding why students at a UC college droped out?

## A. Data Cleaning and Manipulation
```{r, echo = FALSE}
## Rename Variables
rn_df <- df %>%
  rename(Age = Age.at.enrollment,
         Mart.Stat = Marital.status,
         Mom.Occp = Mother.s.occupation,
         Mom.Ed = Mother.s.qualification,
         Dad.Occp = Father.s.occupation,
         Dad.Ed = Father.s.qualification,
         Smstr1.GPA = Curricular.units.1st.sem..grade.,
         Smstr2.GPA = Curricular.units.2nd.sem..grade., 
         Nationality = Nacionality, 
         Scholarship = Scholarship.holder,
         Admission.Grade = Admission.grade)

## Filter Data to a new data frame. These are the variables we're working with for this project.
fltr_df <- rn_df |>
   dplyr::select(Target, Age, 
         Smstr1.GPA, Smstr2.GPA,
         Gender, Mart.Stat, 
         Nationality, 
         Mom.Ed, Mom.Occp,
         Dad.Ed, Dad.Occp, 
         Course,
         Admission.Grade,
         Scholarship) |>
  arrange(Age)  
head(fltr_df)
```

```{r, echo = FALSE}
## Replace semester GPA scale from(0,20) to (0,5)
fltr_df$Smstr1.GPA <- round( (fltr_df$Smstr1.GPA/20)*5, digits = 2)
fltr_df$Smstr2.GPA <- round( (fltr_df$Smstr2.GPA/20)*5, digits = 2)

## Assign each student to an avg grade category that reflects their GPA
## UCI's Grading(http://www.reg.uci.edu/services/transcripts/notations.html)
asgn_Grade <- function(gpa){
  brackets = c("na")
  for(i in 1:length(gpa)){
    if(0<=gpa[i] & gpa[i] <=0.69){
      brackets[i] <- "F"
    }else if(0.7<= gpa[i] & gpa[i] <=0.99){
      brackets[i] <- "D-"
    }else if(1<= gpa[i] & gpa[i] <=1.29){
      brackets[i] <- "D"
    }else if(1.3<= gpa[i] & gpa[i] <=1.69){
      brackets[i] <- "D+"
    }else if(1.7<= gpa[i] & gpa[i] <=1.99){
      brackets[i] <- "C-"
    }else if(2.0<= gpa[i] & gpa[i] <=2.29){
      brackets[i] <- "C"
    }else if(2.3<= gpa[i] & gpa[i] <=2.69){
      brackets[i] <- "C+"
    }else if(2.7<= gpa[i] & gpa[i] <=2.99){
      brackets[i] <- "B-"
    }else if(3.0<= gpa[i] & gpa[i] <=3.29){
      brackets[i] <- "B"
    }else if(3.3<= gpa[i] & gpa[i]<=3.69){
      brackets[i] <- "B+"
    }else if(3.7<= gpa[i] & gpa[i] <=3.99){
      brackets[i] <- "A-"
    }else if(4.0<=gpa[i]){
      brackets[i] <- "A"
    }
  }
  return(brackets)
}

## Assign each student to an age group
asgn_AgeGrp <- function(age){
  brackets <- c("na")
  for(i in 1:length(age)){
    if(17<=age[i] & age[i]<=19){
      brackets[i] <- "A_yng_adults"
    }else if(20<=age[i] & age[i]<=24){
      brackets[i] <- "B_early_20s"
    }else if(25<=age[i] & age[i]<=29){
      brackets[i] <- "C_late_20s"
    }else if(30<=age[i] & age[i]<=34){
      brackets[i] <- "D_early_30s"
    }else if(35<=age[i] & age[i]<=39){
      brackets[i] <- "E_late_30s"
    }else if(40<=age[i] & age[i]<=44){
      brackets[i] <- "F_early_40s"
    }else if(45<=age[i] & age[i]<=49){
      brackets[i] <- "G_late_40s"
    }else if(50<=age[i] & age[i]<=54){
      brackets[i] <- "H_early_50s"
    }else if(55<=age[i] & age[i]<=59){
      brackets[i] <- "I_late_50s"
    }else if(60<=age[i] & age[i]<=64){
      brackets[i] <- "J_early_60s"
    }else if(65<=age[i] & age[i]<=69){
      brackets[i] <- "K_late_60s"
    }else if(70<=age[i]){
      brackets[i] <- "L_over_70"
    }
  }
  return(brackets)
}
fltr_df$Age.Group <- asgn_AgeGrp(fltr_df$Age)

## Create a new variable called GPA
fltr_df <- fltr_df |>
  mutate( GPA = round((Smstr1.GPA + Smstr2.GPA)/2, 2), .after = Smstr2.GPA )

## Remove Outliers/Irrelavent data
#sum(fltr_df$Age>35) ## 418 Total Students over 40 in this data set.
#sum(fltr_df$Age>35)/length(fltr_df$Age) ## 418/4424 = 0.09. So, only 0.09 of the data is removed. This should be fine
## I believe that Students over 40 are irrelevant. So, I am removing them from the dataset.
fltr_df <- fltr_df |>
  filter(Age<35)


## Add a Grade variable that categorizes each student's letter grade with their respected GPA
fltr_df$Grade <- asgn_Grade(fltr_df$GPA)
fltr_df$Grade <- factor(fltr_df$Grade, levels = c("F","D-","D","D+","C-","C","C+","B-","B","B+","A-","A","A+"))

## Commands for seeing the data values
#unique(fltr_df$Mom.Occp)
#max(fltr_df$GPA)
#fltr_df %>% count(Dad.Occp)
```

Write this new data file to a csv file.
```{r, echo = FALSE}
write.csv(fltr_df, "Data.csv")
readTest <- read.csv("Data.csv",header = T, sep = ',')
head(readTest)
```

**Important Variables we're using:** \newline
Age - The student's Age during their enrollment. Many unique values. (ranges from 17 to 34) \newline
Age.Group - The students are categorized by age groups. We're only including 4 age groups. Each group ranges 5 years except the first age group, "Young Adults". \newline
Smstr1.GPA - The student's GPA from their 1st semester. Many unique values. (ranges from 0.0 to 4.72) \newline
Smstr2.GPA - The student's GPA from their 2nd semester. Many unique values. (ranges from 0.0. to 4.64) \newline
GPA - The student's overall GPA. This is calculated by averaging the student's semester 1 & 2 GPA. (ranges from 0.0 to 4.57) \newline
Gender - Male or Female. (2 values) \newline
Mart.Stat - The student's marital status. (6 values) \newline
Nationality - The student's ethnicity or national origin. (21 values) \newline
Mom.Ed - The student's mother's education level. (28 values) \newline
Mom.Occp - The student's mother's occupation or job. (32 values) \newline
Dad.Ed - The student's father's education level. (33 values) \newline
Dad.Occp - The sutdent's father's occupation or job (44 values) \newline
Scholarship - Yes the student is receiving a scholarship or No the student is not receiving a scholarship. (2 values) \newline

**The Mom.Ed, Mom.Occp, Dad.Ed, & Dad.Occp values are listed numerically. I'm not sure how to categorized them. I think some of them can be removed, I will look into these next.**
\
**Mom.Ed values:** \newline
1 - Secondary Education - 12th Year of Schooling or Eq. \newline
2 - Higher Education - Bachelor's Degree \newline
3 - Higher Education - Degree \newline
4 - Higher Education - Master's \newline
5 - Higher Education - Doctorate \newline
6 - Frequency of Higher Education \newline
9 - 12th Year of Schooling - Not Completed \newline
10 - 11th Year of Schooling - Not Completed \newline
11 - 7th Year (Old) \newline
12 - Other - 11th Year of Schooling \newline
14 - 10th Year of Schooling \newline
18 - General commerce course \newline
19 - Basic Education 3rd Cycle (9th/10th/11th Year) or Equiv. \newline
22 - Technical-professional course \newline
26 - 7th year of schooling \newline
27 - 2nd cycle of the general high school course \newline
29 - 9th Year of Schooling - Not Completed \newline
30 - 8th year of schooling \newline 
34 - Unknown \newline
35 - Can't read or write \newline
36 - Can read without having a 4th year of schooling \newline
37 - Basic education 1st cycle (4th/5th year) or equiv. \newline
38 - Basic Education 2nd Cycle (6th/7th/8th Year) or Equiv.\newline 
39 - Technological specialization course \newline
40 - Higher education - degree (1st cycle) \newline
41 - Specialized higher studies course \newline
42 - Professional higher technical course \newline
43 - Higher Education - Master (2nd cycle) \newline
44 - Higher Education - Doctorate (3rd cycle)\newline
\
**Mom.Occp values:** \newline
0 - Student \newline
1 - Representatives of the Legislative Power and Executive Bodies, Directors, Directors and Executive Managers \newline
2 - Specialists in Intellectual and Scientific Activities \newline
3 - Intermediate Level Technicians and Professions \newline
4 - Administrative staff \newline
5 - Personal Services, Security and Safety Workers and Sellers \newline
6 - Farmers and Skilled Workers in Agriculture, Fisheries and Forestry \newline
7 - Skilled Workers in Industry, Construction and Craftsmen \newline
8 - Installation and Machine Operators and Assembly Workers \newline
9 - Unskilled Workers 10 - Armed Forces Professions \newline
90 - Other Situation \newline
99 - (blank) \newline
122 - Health professionals \newline
123 - teachers \newline
125 - Specialists in information and communication technologies (ICT) \newline
131 - Intermediate level science and engineering technicians and professions \newline
132 - Technicians and professionals, of intermediate level of health \newline
134 - Intermediate level technicians from legal, social, sports, cultural and similar services \newline
141 - Office workers, secretaries in general and data processing operators \newline
143 - Data, accounting, statistical, financial services and registry-related operators \newline
144 - Other administrative support staff 151 - personal service workers \newline
152 - sellers \newline
153 - Personal care workers and the like \newline
171 - Skilled construction workers and the like, except electricians \newline
173 - Skilled workers in printing, precision instrument manufacturing, jewelers, artisans and the like \newline
175 - Workers in food processing, woodworking, clothing and other industries and crafts \newline
191 - cleaning workers \newline
192 - Unskilled workers in agriculture, animal production, fisheries and forestry \newline
193 - Unskilled workers in extractive industry, construction, manufacturing and transport \newline
194 - Meal preparation assistants \newline

**Dad.Ed values: ** \\
1 - Secondary Education - 12th Year of Schooling or Eq. \newline
2 - Higher Education - Bachelor's Degree \newline
3 - Higher Education - Degree \newline
4 - Higher Education - Master's \newline
5 - Higher Education - Doctorate \newline
6 - Frequency of Higher Education \newline
9 - 12th Year of Schooling - Not Completed \newline 
10 - 11th Year of Schooling - Not Completed \newline
11 - 7th Year (Old) \newline
12 - Other - 11th Year of Schooling \newline
13 - 2nd year complementary high school course \newline
14 - 10th Year of Schooling \newline
18 - General commerce course \newline
19 - Basic Education 3rd Cycle (9th/10th/11th Year) or Equiv. \newline
20 - Complementary High School Course \newline
22 - Technical-professional course \newline
25 - Complementary High School Course - not concluded \newline
26 - 7th year of schooling \newline
27 - 2nd cycle of the general high school course \newline
29 - 9th Year of Schooling - Not Completed \newline
30 - 8th year of schooling \newline
31 - General Course of Administration and Commerce \newline
33 - Supplementary Accounting and Administration \newline
34 - Unknown \newline
35 - Can't read or write \newline
36 - Can read without having a 4th year of schooling \newline
37 - Basic education 1st cycle (4th/5th year) or equiv. \newline
38 - Basic Education 2nd Cycle (6th/7th/8th Year) or Equiv. \newline
39 - Technological specialization course \newline
40 - Higher education - degree (1st cycle) \newline
41 - Specialized higher studies course \newline
42 - Professional higher technical course \newline
43 - Higher Education - Master (2nd cycle) \newline
44 - Higher Education - Doctorate (3rd cycle) \newline

**Dad.Occp values:** \newline
0 - Student \newline
1 - Representatives of the Legislative Power and Executive Bodies, Directors, Directors and Executive Managers \newline
2 - Specialists in Intellectual and Scientific Activities \newline
3 - Intermediate Level Technicians and Professions \newline
4 - Administrative staff \newline
5 - Personal Services, Security and Safety Workers and Sellers \newline
6 - Farmers and Skilled Workers in Agriculture, Fisheries and Forestry \newline
7 - Skilled Workers in Industry, Construction and Craftsmen \newline
8 - Installation and Machine Operators and Assembly Workers \newline
9 - Unskilled Workers 10 - Armed Forces Professions \newline
90 - Other Situation \newline
99 - (blank) \newline
101 - Armed Forces Officers \newline
102 - Armed Forces Sergeants \newline
103 - Other Armed Forces personnel \newline
112 - Directors of administrative and commercial services \newline
114 - Hotel, catering, trade and other services directors \newline
121 - Specialists in the physical sciences, mathematics, engineering and related techniques \newline
122 - Health professionals \newline
123 - teachers \newline
124 - Specialists in finance, accounting, administrative organization, public and commercial relations \newline
131 - Intermediate level science and engineering technicians and professions \newline
132 - Technicians and professionals, of intermediate level of health \newline
134 - Intermediate level technicians from legal, social, sports, cultural and similar services \newline
135 - Information and communication technology technicians \newline
141 - Office workers, secretaries in general and data processing operators \newline
143 - Data, accounting, statistical, financial services and registry-related operators \newline
144 - Other administrative support staff \newline
151 - personal service workers \newline
152 - sellers \newline
153 - Personal care workers and the like \newline
154 - Protection and security services personnel \newline
161 - Market-oriented farmers and skilled agricultural and animal production workers \newline
163 - Farmers, livestock keepers, fishermen, hunters and gatherers, subsistence \newline
171 - Skilled construction workers and the like, except electricians \newline
172 - Skilled workers in metallurgy, metalworking and similar \newline
174 - Skilled workers in electricity and electronics \newline
175 - Workers in food processing, woodworking, clothing and other industries and crafts \newline
181 - Fixed plant and machine operators \newline
182 - assembly workers \newline
183 - Vehicle drivers and mobile equipment operators \newline
192 - Unskilled workers in agriculture, animal production, fisheries and forestry \newline
193 - Unskilled workers in extractive industry, construction, manufacturing and transport \newline
194 - Meal preparation assistants \newline
195 - Street vendors (except food) and street service providers\newline

# Multiple Logistic Regression Model

We will first create a new data set that we need specifically for our models. We will only consider dropouts and graduates for the sake of the model and the question we are trying to answer (what factors lead to a student dropping out)
```{r, echo = FALSE}
library(MASS)
mod_df <- fltr_df %>%
  filter(Target == "Dropout" | Target == "Graduate")
mod_df$Target <- ifelse(mod_df$Target == "Dropout", 1, 0)
```

This preliminary model has all of our variables. It is not a very good model just based on the fact that some of our predictors have very high p-values. Such high p-values indicate that the variables are not statistically siginificant, so we want a model that only includes variables that are statistically significant. 
```{r}
mod_log <- glm(Target ~ Age + Gender + Mom.Ed + Mom.Occp + Dad.Ed + Dad.Occp + Admission.Grade + Scholarship + Smstr1.GPA + Smstr2.GPA, mod_df, family = binomial)
summary(mod_log)
```
Let's try to select the best predictors using forwards and backwards selection. 

### Backwards selection

We will first use backwards elimination to remove predictors until the AIC value is stable and as low as possible. The AIC value is a measure for accuracy of the model, another metric used to measure how good the model is. We can use the step function to perform backwards elimination.

After performing the backwards elimination, we are left with a model with the following predictors: 

Age, Gender, Mom.Occp, Admission.Grade, Scholarship, Smstr2.GPA
```{r, echo = FALSE}
backward_model <- step(mod_log, direction = "backward")
summary(backward_model)
```
### Forwards selection

We will use forwards elimination to add predictors starting with an empty model until the AIC value is stable and as low as possible.

After performing the forwards elimination, we are left with a model with the following predictors: 

Age, Gender, Mom.Occp, Admission.Grade, Scholarship, Smstr2.GPA

These are the same predictors that we got from our backwards elimination. We will now perform diagnostics on this model to understand where it has flaws. 
```{r, echo = FALSE}
empty_mod <- glm(Target ~ 1, data = mod_df, family = binomial)
forward_model <- stepAIC(empty_mod, scope = list(lower = ~1, upper = ~Age + Gender + Mom.Ed + Mom.Occp + Dad.Ed + Dad.Occp + Admission.Grade + Scholarship + Smstr1.GPA + Smstr2.GPA), direction = "forward")
summary(forward_model)
```

## Final multiple logistic regression model
*will need to add analysis of the models and the coefficients and what they mean*

```{r}
final_log_mod <- forward_model
```


## Model analysis and Diagnostics

Let's look at some model diagnostics and analyze our final model.

### AIC and BIC
Let's first look at the AIC and BIC scores of our model. 
*need analysis*
```{r, echo = FALSE}
AIC(final_log_mod)
BIC(final_log_mod)
```
### Residual plots

*need analysis for what the residual plots mean*
```{r, echo = FALSE}
# Deviance Residuals
dev_res <- residuals(final_log_mod, type = "deviance")

# Pearson Residuals
pearson_res <- residuals(final_log_mod, type = "pearson")

# Create data frame for plotting
res_df <- data.frame(Predicted = predict(final_log_mod),
                     Deviance_Residuals = dev_res,
                     Pearson_Residuals = pearson_res)

# Plot Deviance Residuals vs. Predicted Values
ggplot(res_df, aes(x = Predicted, y = Deviance_Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Scatterplot of Deviance Residuals",
       x = "Predicted Values",
       y = "Deviance Residuals")

# Plot Pearson Residuals vs. Predicted Values
ggplot(res_df, aes(x = Predicted, y = Pearson_Residuals)) +
  geom_point() +
  xlab("Predicted Values") +
  ylab("Pearson Residuals") +
  ggtitle("Pearson Residuals vs. Predicted Values")

# Cook's Distance for Influential Observations
cooks_dist <- cooks.distance(final_log_mod)
cook_df <- data.frame(Cooks_Distance = cooks_dist)

cook_df <- cook_df %>% 
  mutate(Observation = row_number())

# Plot Cook's Distance
ggplot(cook_df, aes(x = Observation, y = Cooks_Distance)) +
  geom_point() +
  geom_hline(yintercept = 4/(nrow(cook_df)-ncol(cook_df)), color = "red", linetype = "dashed") + # Threshold line
  xlab("Observation") +
  ylab("Cook's Distance") +
  ggtitle("Cook's Distance Plot")

```

### Multicollinearity 
```{r, echo = FALSE}
# install.packages("car")
library(car)

# Check Multicollinearity using VIF
vif(final_log_mod)
```

### Leave-One-Out-Cross-Validation

A kappa value of approximately 0.5794997 suggests a moderate level of agreement beyond chance. Our classification model performs very well, achieving an accuracy of around 81.5489% with a moderate level of agreement between predicted and actual classifications.
```{r, echo = FALSE}
# leave one out cross validation 
# install.packages("caret")
library(caret)
set.seed(12345)

loocv_control <- trainControl(method = "LOOCV")

mod_df$Target <- factor(mod_df$Target)

# Train the model using LOOCV
LOOCV_model <- train(
  Target ~ Scholarship + Age + Gender + Admission.Grade + Mom.Occp + Smstr2.GPA,
  data = mod_df,
  method = "glm",
  trControl = loocv_control
)

# Print the results
print(LOOCV_model)
```
### Confusion Matrix

Next, we want to look at a confusion matrix using our model. Based on our F1 value of 0.71, we conclude that our model has reasonably good precision and recall. 
```{r, echo = FALSE}
#Confusion matrix
predicted_probs <- predict(final_log_mod, type = "response")

predicted_classes <- ifelse(predicted_probs > 0.5, "Positive", "Negative")

actual_classes <- mod_df$Target

conf_matrix_log <- table(actual_classes, predicted_classes)
print(conf_matrix_log)
TP <- conf_matrix_log[2, 2]
TN <- conf_matrix_log[1, 1]
FP <- conf_matrix_log[1, 2]
FN <- conf_matrix_log[2, 1]
accuracy <- (TP + TN) / sum(conf_matrix_log)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
specificity <- TN / (TN + FP)
f1_score <- 2 * (precision * recall) / (precision + recall)

print(paste("Accuracy:", round(accuracy, 2)))
print(paste("Precision:", round(precision, 2)))
print(paste("Recall (Sensitivity):", round(recall, 2)))
print(paste("Specificity:", round(specificity, 2)))
print(paste("F1 Score:", round(f1_score, 2)))
```

# Random Forest Model

Let's create a random forest model using all of our predictors next. We will use this model to compare it to our multiple logistic regression model. 

The first thing to note about this model is that after pruning the model using a very high value of importance, it chose the following variables: Age, Admission.Grade, Smstr1.GPA, Smstr2.GPA. This is a very different subset of variables. This could indicate that these subset of variables predicts or target more accurately. We will compare the analytics of this model to our first one to see which one is better. 

```{r}
# Load required packages
#install.packages("randomForest")
library(randomForest)
set.seed(12345)

#training the model
rf_model <- randomForest(Target ~ Age + Gender + Mom.Ed + Mom.Occp + Dad.Ed + Dad.Occp + Admission.Grade + Scholarship + Smstr1.GPA + Smstr2.GPA, data = mod_df)

var_importance <- as.data.frame(importance(rf_model))

selected_predictors <- rownames(var_importance)[var_importance$MeanDecreaseGini > 100]

# Train a new random forest model with selected predictors
rf_model_selected <- randomForest(Target ~ ., data = subset(mod_df, select = c("Target", selected_predictors)))

plot(rf_model_selected)
print(selected_predictors)

```

## Model analysis and Diagnostics

Let's now analyze this forest and compare it to our multiple logistic regression model.

### ROC and AUC Curve

We will first look at the ROC and AUC for our random forest. 

*need analysis for the curve*

```{r, echo = FALSE}
#install.packages("ROCR")
library(ROCR)

rf_probs <- predict(rf_model_selected, mod_df, type = "prob")[, "1"]
rf_pred <- prediction(rf_probs, mod_df$Target)
rf_perf <- performance(rf_pred, "tpr", "fpr")

plot(rf_perf, main = "ROC Curve for Random Forest Model", col = "blue", lwd = 2)

```

### Confusion Matrix

Let's now create a confusion matrix and calculating the different metrics. 

Based on this matrix and the metrics, our F1 score is 0.9798. This value is much higher than our confusion matrix from the logistic model. This indicates an extremely high accuracy and precision from our model. 

```{r, echo = FALSE}
rf_predictions <- predict(rf_model_selected, mod_df, type = "class")
rf_conf_matrix <- confusionMatrix(rf_predictions, mod_df$Target)
print(rf_conf_matrix)
TP <- rf_conf_matrix$table[2, 2]
TN <- rf_conf_matrix$table[1, 1]
FP <- rf_conf_matrix$table[1, 2]
FN <- rf_conf_matrix$table[2, 1]

accuracy <- (TP + TN) / sum(rf_conf_matrix$table)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
specificity <- TN / (TN + FP)
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("Accuracy: ", accuracy, "\n")
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")
cat("Specificity: ", specificity, "\n")
cat("F1 Score: ", f1_score, "\n")
```

### K-Fold Cross Validation

Now, we will use k-fold-cross-validation to train our model and test it using the random forest models. 

We did not use LOOCV as it was not able to run it on the forest. Based on a 20-fold cross validation, we got an accuracy of 82.69% and a kappa value of 0.6136719 Our model performs very well, better than the logistic regression model. The kappa value is also higher, so it also has a higher level of agreement between predicted and actual classifications. 


```{r, echo = FALSE}
set.seed(12345)
# Define the control parameters for LOOCV
kfold_control <- trainControl(method = "cv", number = 20)  # 10-fold cross-validation

# Train the Random Forest model using LOOCV
rf_model_loocv <- train(
  Target ~ Age + Admission.Grade + Smstr1.GPA + Smstr2.GPA,
  data = mod_df,
  method = "rf",
  trControl = kfold_control
)

print(rf_model_loocv)

```

Based on all of these analyses and comparison of the two models, we can conclude that our random forest model is better than our multiple logistic regression model. 

*need more conclusions, etc*
